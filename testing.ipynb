{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d220b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a9e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Conjugate Gradient optimization...\n",
      "Initial gradient norm: 1.099863e+03\n",
      "Iter   0: loss = 6.219922e+02, grad_norm = 3.754382e+02, alpha = 2.786992e-03\n",
      "Iter  10: loss = 3.603968e+02, grad_norm = 3.089551e+01, alpha = 1.097096e-02\n",
      "Iter  20: loss = 3.504162e+02, grad_norm = 1.682805e+01, alpha = 1.628162e-02\n",
      "Iter  30: loss = 3.474951e+02, grad_norm = 6.700162e+00, alpha = 1.461576e-02\n",
      "Iter  40: loss = 3.456140e+02, grad_norm = 4.880370e+00, alpha = 8.886498e-03\n",
      "Iter  50: loss = 3.452259e+02, grad_norm = 3.269530e+00, alpha = 4.973870e-03\n",
      "Iter  60: loss = 3.451422e+02, grad_norm = 2.026267e+00, alpha = 1.936826e-03\n",
      "Iter  70: loss = 3.451041e+02, grad_norm = 1.458859e+00, alpha = 3.829472e-03\n",
      "Iter  80: loss = 3.450871e+02, grad_norm = 1.243321e+00, alpha = 2.633839e-03\n",
      "Iter  90: loss = 3.450713e+02, grad_norm = 6.476348e-01, alpha = 3.256511e-03\n",
      "Iter 100: loss = 3.450669e+02, grad_norm = 4.684044e-01, alpha = 3.813910e-03\n",
      "Iter 110: loss = 3.450645e+02, grad_norm = 2.673619e-01, alpha = 3.503773e-03\n",
      "Iter 120: loss = 3.450634e+02, grad_norm = 1.827984e-01, alpha = 2.936764e-03\n",
      "Iter 130: loss = 3.450631e+02, grad_norm = 1.197817e-01, alpha = 2.837186e-03\n",
      "Iter 140: loss = 3.450629e+02, grad_norm = 1.007661e-01, alpha = 2.622300e-03\n",
      "Iter 150: loss = 3.450628e+02, grad_norm = 4.099226e-02, alpha = 2.283787e-03\n",
      "Iter 160: loss = 3.450628e+02, grad_norm = 2.344294e-02, alpha = 2.764216e-03\n",
      "Iter 170: loss = 3.450628e+02, grad_norm = 1.415942e-02, alpha = 3.268542e-03\n",
      "Iter 180: loss = 3.450628e+02, grad_norm = 9.254100e-03, alpha = 3.509528e-03\n",
      "Iter 190: loss = 3.450628e+02, grad_norm = 4.876822e-03, alpha = 2.779172e-03\n",
      "Optimization completed in 0.060 seconds\n",
      "Final loss: 3.450628e+02\n",
      "Final gradient norm: 2.259978e-03\n",
      "Starting Conjugate Gradient optimization...\n",
      "Initial gradient norm: 2.337510e+03\n",
      "Iter   0: loss = 1.521006e+02, grad_norm = 5.193465e+02, alpha = 6.384437e-04\n",
      "Iter  10: loss = 2.359419e+01, grad_norm = 2.067195e+01, alpha = 4.023978e-03\n",
      "Iter  20: loss = 2.088603e+01, grad_norm = 7.965754e+00, alpha = 8.442487e-03\n",
      "Iter  30: loss = 2.021356e+01, grad_norm = 2.952424e+00, alpha = 5.236785e-03\n",
      "Iter  40: loss = 2.006117e+01, grad_norm = 1.576080e+00, alpha = 6.078483e-03\n",
      "Iter  50: loss = 1.997670e+01, grad_norm = 7.352284e-01, alpha = 4.891603e-03\n",
      "Iter  60: loss = 1.996981e+01, grad_norm = 2.501700e-01, alpha = 8.653928e-03\n",
      "Iter  70: loss = 1.996866e+01, grad_norm = 8.779542e-02, alpha = 1.095933e-02\n",
      "Iter  80: loss = 1.996846e+01, grad_norm = 5.182639e-02, alpha = 1.544589e-02\n",
      "Iter  90: loss = 1.996842e+01, grad_norm = 1.653120e-02, alpha = 4.881484e-03\n",
      "Iter 100: loss = 1.996841e+01, grad_norm = 7.188332e-03, alpha = 6.867875e-03\n",
      "Iter 110: loss = 1.996841e+01, grad_norm = 2.441745e-03, alpha = 7.080570e-03\n",
      "Iter 120: loss = 1.996841e+01, grad_norm = 1.533076e-03, alpha = 9.460298e-03\n",
      "Iter 130: loss = 1.996841e+01, grad_norm = 4.832443e-04, alpha = 4.627100e-03\n",
      "Iter 140: loss = 1.996841e+01, grad_norm = 1.780565e-04, alpha = 3.365738e-03\n",
      "Iter 150: loss = 1.996841e+01, grad_norm = 7.731340e-05, alpha = 3.941182e-03\n",
      "Iter 160: loss = 1.996841e+01, grad_norm = 3.501668e-05, alpha = 5.826952e-03\n",
      "Iter 170: loss = 1.996841e+01, grad_norm = 2.044832e-05, alpha = 1.193799e-02\n",
      "Iter 180: loss = 1.996841e+01, grad_norm = 6.123589e-06, alpha = 5.831145e-03\n",
      "Iter 190: loss = 1.996841e+01, grad_norm = 1.900283e-06, alpha = 5.501491e-03\n",
      "Converged at iteration 196, gradient norm: 8.919112e-07\n",
      "Optimization completed in 0.032 seconds\n",
      "Final loss: 1.996841e+01\n",
      "Final gradient norm: 8.919112e-07\n",
      "  solver  syn_accuracy  syn_time_s  real_accuracy  real_time_s\n",
      "0     cg      0.886667    0.060002       0.959064        0.032\n",
      "1  lbfgs      0.886667    0.018999       0.959064        0.009\n",
      "\n",
      "Synthetic Data \n",
      "Starting Conjugate Gradient optimization...\n",
      "Initial gradient norm: 1.099863e+03\n",
      "Iter   0: loss = 6.219922e+02, grad_norm = 3.754382e+02, alpha = 2.786992e-03\n",
      "Iter  10: loss = 3.603968e+02, grad_norm = 3.089551e+01, alpha = 1.097096e-02\n",
      "Iter  20: loss = 3.504162e+02, grad_norm = 1.682805e+01, alpha = 1.628162e-02\n",
      "Iter  30: loss = 3.474951e+02, grad_norm = 6.700162e+00, alpha = 1.461576e-02\n",
      "Iter  40: loss = 3.456140e+02, grad_norm = 4.880370e+00, alpha = 8.886498e-03\n",
      "Iter  50: loss = 3.452259e+02, grad_norm = 3.269530e+00, alpha = 4.973870e-03\n",
      "Iter  60: loss = 3.451422e+02, grad_norm = 2.026267e+00, alpha = 1.936826e-03\n",
      "Iter  70: loss = 3.451041e+02, grad_norm = 1.458859e+00, alpha = 3.829472e-03\n",
      "Iter  80: loss = 3.450871e+02, grad_norm = 1.243321e+00, alpha = 2.633839e-03\n",
      "Iter  90: loss = 3.450713e+02, grad_norm = 6.476348e-01, alpha = 3.256511e-03\n",
      "Iter 100: loss = 3.450669e+02, grad_norm = 4.684044e-01, alpha = 3.813910e-03\n",
      "Iter 110: loss = 3.450645e+02, grad_norm = 2.673619e-01, alpha = 3.503773e-03\n",
      "Iter 120: loss = 3.450634e+02, grad_norm = 1.827984e-01, alpha = 2.936764e-03\n",
      "Iter 130: loss = 3.450631e+02, grad_norm = 1.197817e-01, alpha = 2.837186e-03\n",
      "Iter 140: loss = 3.450629e+02, grad_norm = 1.007661e-01, alpha = 2.622300e-03\n",
      "Iter 150: loss = 3.450628e+02, grad_norm = 4.099226e-02, alpha = 2.283787e-03\n",
      "Iter 160: loss = 3.450628e+02, grad_norm = 2.344294e-02, alpha = 2.764216e-03\n",
      "Iter 170: loss = 3.450628e+02, grad_norm = 1.415942e-02, alpha = 3.268542e-03\n",
      "Iter 180: loss = 3.450628e+02, grad_norm = 9.254100e-03, alpha = 3.509528e-03\n",
      "Iter 190: loss = 3.450628e+02, grad_norm = 4.876822e-03, alpha = 2.779172e-03\n",
      "Optimization completed in 0.061 seconds\n",
      "Final loss: 3.450628e+02\n",
      "Final gradient norm: 2.259978e-03\n",
      "\n",
      "Solver = cg\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.85      0.89       157\n",
      "           1       0.85      0.92      0.89       143\n",
      "\n",
      "    accuracy                           0.89       300\n",
      "   macro avg       0.89      0.89      0.89       300\n",
      "weighted avg       0.89      0.89      0.89       300\n",
      "\n",
      "\n",
      "Solver = lbfgs\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.85      0.89       157\n",
      "           1       0.85      0.92      0.89       143\n",
      "\n",
      "    accuracy                           0.89       300\n",
      "   macro avg       0.89      0.89      0.89       300\n",
      "weighted avg       0.89      0.89      0.89       300\n",
      "\n",
      "\n",
      "Real Data:\n",
      "Starting Conjugate Gradient optimization...\n",
      "Initial gradient norm: 1.145832e+03\n",
      "Iter   0: loss = 3.730552e+02, grad_norm = 6.518395e+02, alpha = 2.230626e-03\n",
      "Iter  10: loss = 1.673049e+02, grad_norm = 1.632277e+01, alpha = 2.333497e-02\n",
      "Iter  20: loss = 1.519602e+02, grad_norm = 2.445954e+00, alpha = 6.460343e-03\n",
      "Iter  30: loss = 1.517730e+02, grad_norm = 1.342555e+00, alpha = 4.321755e-03\n",
      "Iter  40: loss = 1.517504e+02, grad_norm = 4.762091e-01, alpha = 6.871531e-03\n",
      "Iter  50: loss = 1.517435e+02, grad_norm = 4.918875e-01, alpha = 2.812444e-03\n",
      "Iter  60: loss = 1.517405e+02, grad_norm = 2.266866e-01, alpha = 2.908859e-03\n",
      "Iter  70: loss = 1.517396e+02, grad_norm = 1.531367e-01, alpha = 8.630192e-03\n",
      "Iter  80: loss = 1.517395e+02, grad_norm = 8.880410e-03, alpha = 1.168985e-02\n",
      "Iter  90: loss = 1.517395e+02, grad_norm = 4.103568e-03, alpha = 5.018898e-03\n",
      "Iter 100: loss = 1.517395e+02, grad_norm = 8.697083e-04, alpha = 2.032913e-03\n",
      "Iter 110: loss = 1.517395e+02, grad_norm = 4.590066e-04, alpha = 5.066486e-03\n",
      "Iter 120: loss = 1.517395e+02, grad_norm = 8.700319e-05, alpha = 2.781552e-02\n",
      "Iter 130: loss = 1.517395e+02, grad_norm = 2.421492e-05, alpha = 3.334130e-02\n",
      "Iter 140: loss = 1.517395e+02, grad_norm = 4.927275e-06, alpha = 2.469518e-03\n",
      "Iter 150: loss = 1.517395e+02, grad_norm = 2.140528e-06, alpha = 2.720480e-03\n",
      "Restarting CG at iteration 158 (non-descent direction)\n",
      "Restarting CG at iteration 159 (non-descent direction)\n",
      "Iter 160: loss = 1.517395e+02, grad_norm = 1.550244e-06, alpha = 1.953125e-03\n",
      "Restarting CG at iteration 160 (non-descent direction)\n",
      "Converged at iteration 162, gradient norm: 5.637425e-07\n",
      "Optimization completed in 0.038 seconds\n",
      "Final loss: 1.517395e+02\n",
      "Final gradient norm: 5.637425e-07\n",
      "\n",
      "Solver = cg\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        63\n",
      "           1       0.96      1.00      0.98       108\n",
      "\n",
      "    accuracy                           0.98       171\n",
      "   macro avg       0.98      0.97      0.97       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n",
      "\n",
      "Solver = lbfgs\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        63\n",
      "           1       0.96      1.00      0.98       108\n",
      "\n",
      "    accuracy                           0.98       171\n",
      "   macro avg       0.98      0.97      0.97       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hucu\\Desktop\\Optimization\\algorithms.py:219: RuntimeWarning: CG did not converge. Final gradient norm: 2.259978e-03 > 1e-06\n",
      "  warnings.warn(\n",
      "c:\\Users\\hucu\\Desktop\\Optimization\\algorithms.py:82: LineSearchWarning: The line search algorithm did not converge\n",
      "  result = line_search(\n",
      "c:\\Users\\hucu\\Desktop\\Optimization\\algorithms.py:219: RuntimeWarning: CG did not converge. Final gradient norm: 2.259978e-03 > 1e-06\n",
      "  warnings.warn(\n",
      "c:\\Users\\hucu\\Desktop\\Optimization\\algorithms.py:82: LineSearchWarning: The line search algorithm did not converge\n",
      "  result = line_search(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import make_classification, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "def test_solvers():\n",
    "    X_syn, y_syn = make_classification(\n",
    "        n_samples=1000, n_features=100, \n",
    "        n_informative=50, n_redundant=2,\n",
    "        n_classes=2, random_state=42, class_sep=2.0,\n",
    "    )\n",
    "\n",
    "    y_syn = 2 * y_syn - 1\n",
    "    data = load_breast_cancer()\n",
    "    X_real, y_real = data.data, data.target\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_syn = scaler.fit_transform(X_syn)\n",
    "    X_real = scaler.fit_transform(X_real)\n",
    "\n",
    "    X_syn_train, X_syn_test, y_syn_train, y_syn_test = train_test_split(\n",
    "        X_syn, y_syn, test_size=0.3, random_state=1)\n",
    "    X_real_train, X_real_test, y_real_train, y_real_test = train_test_split(\n",
    "        X_real, y_real, test_size=0.3, random_state=1)\n",
    "\n",
    "    solvers = ['cg', 'lbfgs']\n",
    "    results = []\n",
    "\n",
    "    for solver in solvers:\n",
    "        clf = SquaredHingeClassifier(C=1.0, solver=solver)\n",
    "\n",
    "\n",
    "        #synthetic data\n",
    "        t0 = time.time()\n",
    "        clf.fit(X_syn_train, y_syn_train)\n",
    "        t_syn = time.time() - t0\n",
    "        y_syn_pred = clf.predict(X_syn_test)\n",
    "        acc_syn = accuracy_score(y_syn_test, y_syn_pred)\n",
    "\n",
    "        #real data\n",
    "        t0 = time.time()\n",
    "        clf.fit(X_real_train, y_real_train)\n",
    "        t_real = time.time() - t0\n",
    "        y_real_pred = clf.predict(X_real_test)\n",
    "        acc_real = accuracy_score(y_real_test, y_real_pred)\n",
    "\n",
    "        results.append({\n",
    "            'solver': solver,\n",
    "            'syn_accuracy': acc_syn,\n",
    "            'syn_time_s': t_syn,\n",
    "            'real_accuracy': acc_real,\n",
    "            'real_time_s': t_real\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df)\n",
    "    print(\"\\nSynthetic Data \")\n",
    "    for solver in solvers:\n",
    "        clf = SquaredHingeClassifier(C=1.0, solver=solver)\n",
    "        clf.fit(X_syn_train, y_syn_train)\n",
    "        print(f\"\\nSolver = {solver}\")\n",
    "        print(classification_report(y_syn_test, clf.predict(X_syn_test)))\n",
    "    \n",
    "    print(\"\\nReal Data:\")\n",
    "    for solver in solvers:\n",
    "        clf = SquaredHingeClassifier(C=1.0, solver=solver)\n",
    "        clf.fit(X_real_train, y_real_train)\n",
    "        print(f\"\\nSolver = {solver}\")\n",
    "        print(classification_report(y_real_test, clf.predict(X_real_test)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_solvers()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
